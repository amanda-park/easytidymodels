#View how model metrics for RMSE, R-Squared, and MAE look for training data
xgReg$trainScore
#Visualize testing data and its predictions
xgReg$testPred %>% select(.pred, !!resp)
#View how model metrics for RMSE, R-Squared, and MAE look for testing data
xgReg$testScore
#See the final model chosen by xg based on optimizing for your chosen evaluation metric
xgReg$final
#See how model fit looks based on another evaluation metric
xgReg$tune %>% tune::select_best("rmse")
#See feature importance of model
xgReg$featImpPlot
#See the final model chosen by mars based on optimizing for your chosen evaluation metric
marsReg$final
#See how model fit looks based on another evaluation metric
marsReg$tune %>% tune::show_best("mae")
# Finalize parameters
rfParam <- rfReg$tune %>% show_best("rmse", n=1) %>%
select(mtry, min_n)
xgParam <- xgReg$tune %>%
show_best("rmse", n=1) %>%
select(mtry:sample_size)
# Collect model predictions to stack
xgStack <- xgReg$tune %>%
collect_predictions() %>%
inner_join(xgParam) %>%
select(id, .row, !!resp, xgboost = .pred)
rfStack <- rfReg$tune %>%
collect_predictions() %>%
inner_join(rfParam) %>%
select(id, .row, randomforest = .pred)
marsStack <- mars$tune %>%
collect_predictions() %>%
select(id, .row, mars = .pred)
marsReg <- marsRegress(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "rmse"
)
#Visualize training data and its predictions
marsReg$trainPred %>% select(.pred, !!resp)
#View how model metrics for RMSE, R-Squared, and MAE look for training data
marsReg$trainScore
#Visualize testing data and its predictions
marsReg$testPred %>% select(.pred, !!resp)
#View how model metrics for RMSE, R-Squared, and MAE look for testing data
marsReg$testScore
#See the final model chosen by mars based on optimizing for your chosen evaluation metric
marsReg$final
#See how model fit looks based on another evaluation metric
marsReg$tune %>% tune::show_best("mae")
marsStack <- mars$tune %>%
collect_predictions() %>%
select(id, .row, mars = .pred)
marsStack <- marsReg$tune %>%
collect_predictions() %>%
select(id, .row, mars = .pred)
knnStack <- knnReg$tune %>%
collect_predictions() %>%
select(id, .row, knn = .pred)
svmStack <- svmReg$tune %>%
collect_predictions() %>%
select(id, .row, knn = .pred)
lmStack <- linearReg$tune %>%
collect_predictions() %>%
select(id, .row, knn = .pred)
lmStack <- linearReg$tune %>%
collect_predictions() %>%
select(id, .row, knn = .pred)
stackDat <- xgStack %>%
left_join(rfStack) %>%
left_join(marsStack) %>%
left_join(knnStack) %>%
left_join(svmStack) %>%
left_join(lmStack) %>%
select(-id, -.row)
stackModel <- linear_reg(penalty = .7, mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet") %>%
fit(da~., data = stackDat)
stackModel <- linear_reg(penalty = .7, mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet") %>%
fit(Sale_Price ~ ., data = stackDat)
stackModel %>% tidy()
stackDat <- xgStack %>%
left_join(rfStack) %>%
left_join(marsStack) %>%
left_join(knnStack) %>%
left_join(svmStack) %>%
left_join(lmStack) %>%
select(-id, -.row)
stackDat
svmStack <- svmReg$tune %>%
collect_predictions() %>%
select(id, .row, svm = .pred)
lmStack <- linearReg$tune %>%
collect_predictions() %>%
select(id, .row, lm = .pred)
lmStack <- linearReg$tune %>%
collect_predictions() %>%
select(id, .row, lm = .pred)
stackDat <- xgStack %>%
left_join(rfStack) %>%
left_join(marsStack) %>%
left_join(knnStack) %>%
left_join(svmStack) %>%
left_join(lmStack) %>%
select(-id, -.row)
stackModel <- linear_reg(penalty = .5, mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet") %>%
fit(Sale_Price ~ ., data = stackDat)
stackModel <- linear_reg(penalty = .5, mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet") %>%
fit(formula, data = stackDat)
stackModel %>% tidy()
stackModel <- linear_reg(penalty = .2, mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet") %>%
fit(formula, data = stackDat)
stackModel %>% tidy()
xgFinal <- xgReg$final %>% last_fit(splitDat$split)
rfFinal <- rfReg$final %>% last_fit(splitDat$split)
xgFinal <- xgReg$final %>% last_fit(split$split)
xgFinal <- xgReg$final %>% last_fit(split$split)
rfFinal <- rfReg$final %>% last_fit(split$split)
marsFinal <- mars$final %>% last_fit(split$split)
marsFinal <- marsReg$final %>% last_fit(split$split)
knnFinal <- knnReg$final %>% last_fit(split$split)
svmFinal <- svmReg$final %>% last_fit(split$split)
lmFinal <- linearReg$final %>% last_fit(split$split)
stackFinal <- tibble("model" = list(xgFinal, rfFinal, marsFinal, knnFinal, svmFinal, lmFinal),
"model_names" = c("xgboost", "randomforest", "mars", "knn", "svm", "lm")) %>%
mutate(pred = map(model, collect_predictions))
stackFinal <- stackFinal %>%
select(model_names, pred) %>%
unnest(pred) %>%
pivot_wider(names_from = model_names, values_from = .pred) %>%
select(-id, -.row, -.config)
predict(stackModel, stackFinal) %>%
bind_cols(stackFinal) %>%
rename("stack" = .pred) %>%
pivot_longer(-!!resp) %>%
group_by(name) %>%
rmse(truth = !!resp, estimate = value) %>%
ungroup() %>%
pivot_wider(names_from = .metric, values_from = .estimate) %>%
arrange(rmse)
pkgdown::build_articles()
?control_grid
devtools::install()
set.seed(24)
# install.packages("devtools")
# devtools::install_github("amanda-park/easytidymodels")
library(easytidymodels)
library(recipes)
library(doParallel)
data(ames, package = "modeldata")
#Use parallel compute to speed up processing time
cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = cores)
#Define your response variable and formula object here
resp <- "Sale_Price"
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Split data into training and testing sets
split <- trainTestSplit(ames,
stratifyOnResponse = TRUE,
responseVar = resp)
#Create recipe for feature engineering for dataset, varies based on data working with
rec <- recipe(formula, data = split$train) %>%
step_log(resp, base = 10) %>%
step_YeoJohnson(Lot_Area, Gr_Liv_Area) %>%
step_other(Neighborhood, threshold = .1)  %>%
step_dummy(all_nominal()) %>%
step_zv(all_predictors()) %>%
step_nzv(all_predictors()) %>%
step_corr(all_numeric(), -all_outcomes(), threshold = .8) %>%
prep()
train_df <- bake(rec, split$train)
test_df <- bake(rec, split$test)
folds <- cvFolds(train_df)
knnReg <- knnRegress(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "rmse"
)
#Visualize training data and its predictions
knnReg$trainPred %>% select(.pred, !!resp)
#View how model metrics for RMSE, R-Squared, and MAE look for training data
knnReg$trainScore
#Visualize testing data and its predictions
knnReg$testPred %>% select(.pred, !!resp)
#View how model metrics for RMSE, R-Squared, and MAE look for testing data
knnReg$testScore
#See the final model chosen by KNN based on optimizing for your chosen evaluation metric
knnReg$final
#See how model fit looks based on another evaluation metric
knnReg$tune %>% tune::show_best("mae")
require(devtools)
devtools::check()
use_r("knnClassif")
?nearest_neighbor
devtools::install()
library(easytidymodels)
#Simulate data
df <- data.frame(var1 = as.factor(c(rep(1, 50), rep(0, 50))),
var2 = rnorm(100),
var3 = c(rnorm(55), rnorm(45, 5)),
var4 = rnorm(100),
var5 = c(rnorm(60), rnorm(40, 3)),
var6 = as.factor(c(rep(0, 20), rep(1, 40), rep(2, 40))))
#Set response variable
resp <- "var6"
split <- trainTestSplit(data = df,
responseVar = resp)
#Create simple recipe object
rec <- createRecipe(split$train,
responseVar = resp)
#Create training, testing, and bootstrapped data sets
train_df <- recipes::bake(rec, split$train)
test_df <- recipes::bake(rec, split$test)
boot_df <- split$boot
#Create cross-validation folds
folds <- cvFolds(train_df, 5)
knnClass <- knnClassif(
recipe = rec,
response = resp,
folds = cvFolds,
train = trainSet,
test = testSet,
evalMetric = "bal_accuracy"
)
devtools::load_all()
document()
devtools::check()
install()
knitr::opts_chunk$set(echo = TRUE)
#Set working directory and load functions in
#setwd("C:/Users/youid/Dropbox/Professional")
source("TidyModelsFunctions.R")
require(pacman)
p_load(
tidymodels,
vip,
fastshap,
doParallel,
xgboost,
glmnet,
earth,
kknn,
patchwork,
margins,
ggfortify,
modeltime,
timetk,
modeltime.ensemble,
modeltime.resample,
kernlab,
update = FALSE
)
knnClass <- knnClassif(
recipe = rec,
response = resp,
folds = cvFolds,
train = trainSet,
test = testSet,
evalMetric = "bal_accuracy"
)
library(easytidymodels)
#Simulate data
df <- data.frame(var1 = as.factor(c(rep(1, 50), rep(0, 50))),
var2 = rnorm(100),
var3 = c(rnorm(55), rnorm(45, 5)),
var4 = rnorm(100),
var5 = c(rnorm(60), rnorm(40, 3)),
var6 = as.factor(c(rep(0, 20), rep(1, 40), rep(2, 40))))
#Set response variable
resp <- "var6"
split <- trainTestSplit(data = df,
responseVar = resp)
#Create simple recipe object
rec <- createRecipe(split$train,
responseVar = resp)
#Create training, testing, and bootstrapped data sets
train_df <- recipes::bake(rec, split$train)
test_df <- recipes::bake(rec, split$test)
boot_df <- split$boot
#Create cross-validation folds
folds <- cvFolds(train_df, 5)
knnClass <- knnClassif(
recipe = rec,
response = resp,
folds = cvFolds,
train = trainSet,
test = testSet,
evalMetric = "bal_accuracy"
)
?knnClassif
document()
?knnClassifc
?knnClassif
install()
library(easytidymodels)
#Simulate data
df <- data.frame(var1 = as.factor(c(rep(1, 50), rep(0, 50))),
var2 = rnorm(100),
var3 = c(rnorm(55), rnorm(45, 5)),
var4 = rnorm(100),
var5 = c(rnorm(60), rnorm(40, 3)),
var6 = as.factor(c(rep(0, 20), rep(1, 40), rep(2, 40))))
#Set response variable
resp <- "var6"
split <- trainTestSplit(data = df,
responseVar = resp)
#Create simple recipe object
rec <- createRecipe(split$train,
responseVar = resp)
#Create training, testing, and bootstrapped data sets
train_df <- recipes::bake(rec, split$train)
test_df <- recipes::bake(rec, split$test)
boot_df <- split$boot
#Create cross-validation folds
folds <- cvFolds(train_df, 5)
knnClass <- knnClassif(
recipe = rec,
response = resp,
folds = cvFolds,
train = trainSet,
test = testSet,
evalMetric = "bal_accuracy"
)
knnClass <- knnClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "bal_accuracy"
)
knnClass$testConfMatPlot
use_r("svmClassif")
document()
install()
svmClass <- svmClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "bal_accuracy"
)
svmClass$testConfMatPlot
build_readme()
build_readme()
pkgdown::preview_site()
usethis::use_vignette("easytidymodels-classification")
?modeldata
require(modeldata)
?modeldata
??modeldata
set.seed(24)
# install.packages("devtools")
# devtools::install_github("amanda-park/easytidymodels")
library(easytidymodels)
library(recipes)
library(doParallel)
data(penguins, package = "modeldata")
#Use parallel compute to speed up processing time
cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = cores)
#Define your response variable and formula object here
resp <- "sex"
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Split data into training and testing sets
split <- trainTestSplit(penguins,
stratifyOnResponse = TRUE,
responseVar = resp)
#Create recipe for feature engineering for dataset, varies based on data working with
rec <- recipe(formula, data = split$train) %>%
step_knnimpute(!!resp) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_predictors()) %>%
step_normalize(all_predictors())
step_dummy(all_nominal(), -all_outcomes()) %>%
step_zv(all_predictors()) %>%
step_nzv(all_predictors()) %>%
step_corr(all_numeric(), -all_outcomes(), threshold = .8) %>%
prep()
#Define your response variable and formula object here
resp <- "sex"
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Split data into training and testing sets
split <- trainTestSplit(penguins,
stratifyOnResponse = TRUE,
responseVar = resp)
#Create recipe for feature engineering for dataset, varies based on data working with
rec <- recipe(formula, data = split$train) %>%
step_knnimpute(!!resp) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_zv(all_predictors()) %>%
step_nzv(all_predictors()) %>%
step_corr(all_numeric(), -all_outcomes(), threshold = .8) %>%
prep()
train_df <- bake(rec, split$train)
test_df <- bake(rec, split$test)
folds <- cvFolds(train_df)
knnClass <- knnClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df
)
#Visualize training data and its predictions
knnClass$trainPred %>% select(.pred, !!resp)
knnClass$trainPred
#View how model metrics for RMSE, R-Squared, and MAE look for training data
knnClass$trainScore
#Visualize testing data and its predictions
knnClass$testPred %>% select(.pred_class, !!resp)
#View how model metrics for RMSE, R-Squared, and MAE look for testing data
knnClass$testScore
#See the final model chosen by KNN based on optimizing for your chosen evaluation metric
knnClass$final
#See how model fit looks based on another evaluation metric
knnClass$tune %>% tune::show_best("roc_auc")
lr <- logRegBinary(recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df)
#
lr$trainConfMat
#Plot of confusion matrix
lr$trainConfMatPlot
#Test Confusion Matrix
lr$testConfMat
#Test Confusion Matrix Plot
lr$testConfMatPlot
svmClass <- svmClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df
)
svmClass <- svmClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
metric = "bal_accuracy"
)
svmClass <- svmClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "bal_accuracy"
)
#Visualize training data and its predictions
svmClass$trainConfMat
#View how model metrics for RMSE, R-Squared, and MAE look for training data
svmClass$trainScore
#Visualize testing data and its predictions
svmClass$testConfMat
#View how model metrics for RMSE, R-Squared, and MAE look for testing data
svmClass$testScore
#See the final model chosen by svm based on optimizing for your chosen evaluation metric
svmClass$final
#See how model fit looks based on another evaluation metric
svmClass$tune %>% tune::show_best("roc_auc")
pkgdown::build_articles()
xgClass <- xgBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
#All the same functions for logistic regression work here, but also others:
#Feature importance plot
xgClass$featImpPlot
#Feature importance variables
xgClass$featImpVars
https://usethis.r-lib.org/reference/use_github_action.html
usethis::use_github_action("pkgdown")
knit_readme()
build_readme()
build_readme()
deploy_site_github()
pkgdown::deploy_site_github()
pkgdown::preview_site()
pkgdown::build_articles()
pkgdown::build_reference()
pkgdown::build_home()
pkgdown::build_tutorials()
pkgdown::preview_site()
