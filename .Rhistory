require(pacman)
p_load(easytidymodels,
tidyverse,
recipes,
tidymodels,
nflfastR,
dlookr,
explore)
seasons <- 2007
dat <- purrr::map_df(seasons, function(x) {
readRDS(
url(
glue::glue("https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds")
)
)
})
View(dat)
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
require(pacman)
p_load(easytidymodels,
tidyverse,
recipes,
tidymodels,
nflfastR,
dlookr,
explore,
janitor)
seasons <- 2007
dat <- purrr::map_df(seasons, function(x) {
readRDS(
url(
glue::glue("https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds")
)
)
})
nyDat <- dat %>%
clean_names()
nyDat <- dat %>%
clean_names() %>%
filter(home_team = "NYG" | away_team = "NYG")
nyDat <- dat %>%
clean_names() %>%
filter(home_team == "NYG" | away_team == "NYG")
View(nyDat)
nyOff <- nyDat %>%
filter(posteam == "NYG")
View(nyOff)
nyOff <- nyDat %>%
filter(posteam == "NYG") %>%
select(week, game_seconds_remaining, down, play_type, yards_gained, qb_dropback, pass_length, yards_after_catch, score_differential, incomplete_pass, interception, fumble, rush_attempt, pass_attempt, qb_hit, sack, touchdown, drive_first_downs)
nyOff <- nyDat %>%
filter(posteam == "NYG") %>%
select(week, game_seconds_remaining, down, play_type, yards_gained, qb_dropback, pass_length, yards_after_catch, score_differential, incomplete_pass, interception, fumble, rush_attempt, pass_attempt, qb_hit, sack, touchdown, drive_first_downs) %>%
group_by(week) %>%
summarize(count = n())
set.seed(24)
# install.packages("devtools")
devtools::install_github("amanda-park/easytidymodels")
library(easytidymodels)
library(recipes)
library(doParallel)
data(penguins, package = "modeldata")
#Use parallel compute to speed up processing time
cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = cores)
#Define your response variable and formula object here
resp <- "sex"
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Split data into training and testing sets
split <- trainTestSplit(penguins,
stratifyOnResponse = TRUE,
responseVar = resp)
#Create recipe for feature engineering for dataset, varies based on data working with
rec <- recipe(formula, data = split$train) %>%
step_knnimpute(!!resp) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_medianimpute(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_nzv(all_predictors()) %>%
step_corr(all_numeric(), -all_outcomes(), threshold = .8) %>%
prep()
train_df <- bake(rec, split$train)
test_df <- bake(rec, split$test)
#df <- rbind(train_df, test_df)
folds <- cvFolds(train_df)
xgClass <- xgBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
#Visualize training data and its predictions
xgClass$trainConfMat
#View model metrics for accuracy and kappa
xgClass$trainScore
#Visualize testing data and its predictions
xgClass$testConfMat
#View model metrics for accuracy and kappa
xgClass$testScore
#See the final model chosen by svm based on optimizing for your chosen evaluation metric
xgClass$final
#See how model fit looks based on another evaluation metric
xgClass$tune %>% tune::show_best("bal_accuracy")
# ROC-AUC on CV Folds Example
res <- xgClass$tune %>%
collect_predictions() %>%
group_by(id) %>%
roc_curve(!!resp, .pred_female)
# ROC-AUC on CV Folds Example
res <- xgClass$tune %>%
collect_predictions() %>%
group_by(id) %>%
yardstick::roc_curve(!!resp, .pred_female)
require(tidymodels)
# ROC-AUC on CV Folds Example
res <- xgClass$tune %>%
collect_predictions() %>%
group_by(id) %>%
yardstick::roc_curve(!!resp, .pred_female)
ggplot(res, aes(1 - specificity, sensitivity, color = id)) +
geom_abline(lty = 2, color = "gray80", size = 1.5) +
geom_path(show.legend = TRUE, alpha = 0.6, size = 1.2) +
coord_equal()
# ROC-AUC on CV Folds Example
xgClass$tune %>%
collect_predictions() %>%
group_by(id) %>%
yardstick::roc_curve(!!resp, .pred_female) %>%
ggplot(res, aes(1 - specificity, sensitivity, color = id)) +
geom_abline(lty = 2, color = "gray80", size = 1.5) +
geom_path(show.legend = TRUE, alpha = 0.6, size = 1.2) +
coord_equal()
# ROC-AUC on CV Folds Example
xgClass$tune %>%
collect_predictions() %>%
group_by(id) %>%
yardstick::roc_curve(!!resp, .pred_female) %>%
ggplot(aes(1 - specificity, sensitivity, color = id)) +
geom_abline(lty = 2, color = "gray80", size = 1.5) +
geom_path(show.legend = TRUE, alpha = 0.6, size = 1.2) +
coord_equal()
xgClass$tune %>%
collect_predictions() %>%
group_by(id)
install.packages("treesnip")
remotes::install_github("curso-r/treesnip")
#remotes::install_github("curso-r/treesnip")
mod <- parsnip::boost_tree(mtry = 1, trees = 50) %>%
set_engine("lightgbm")
#remotes::install_github("curso-r/treesnip")
require(treesnip)
mod <- parsnip::boost_tree(mtry = 1, trees = 50) %>%
parsnip::set_engine("lightgbm")
lgbmBinaryClassif <- function(gridNumber = 10,
recipe = rec,
folds = cvFolds,
train = datTrain,
test = datTest,
response = response,
treeNum = 100,
calcFeatImp = TRUE,
evalMetric = "bal_accuracy") {
formula <- stats::as.formula(paste(response, ".", sep="~"))
#Create model
mod <- parsnip::boost_tree(mtry = tune(),
trees = treeNum) %>%
parsnip::set_engine("lightgbm")
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::finalize(dials::mtry(),dplyr::select(train,-matches(response)), force = TRUE),
)
#Set XGBoost grid
grid <- dials::grid_max_entropy(params, size = gridNumber)
wflow <- workflowFunc(mod = mod,
formula = formula,
folds = folds,
grid = grid,
evalMetric = evalMetric,
type = "binary class")
output <- trainTestEvalClassif(final = wflow$final,
train = train,
test = test,
response = response)
output$tune <- wflow$tune
if(calcFeatImp == TRUE) {
featImp <- featImpCalc(final = wflow$final,
train = train,
response = response,
evalMetric = evalMetric)
#Add variables to output
output$featImpPlot <- featImp$plot
output$featImpVars <- featImp$vars
}
return(output)
}
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
#' @importFrom magrittr "%>%"
lgbmBinaryClassif <- function(gridNumber = 10,
recipe = rec,
folds = cvFolds,
train = datTrain,
test = datTest,
response = response,
treeNum = 100,
calcFeatImp = TRUE,
evalMetric = "bal_accuracy") {
formula <- stats::as.formula(paste(response, ".", sep="~"))
#Create model
mod <- parsnip::boost_tree(min_n = tune(),
tree_depth = tune(),
mtry = tune(),
trees = treeNum) %>%
parsnip::set_engine("lightgbm")
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::min_n(),
dials::tree_depth(),
dials::finalize(dials::mtry(),dplyr::select(train,-matches(response)), force = TRUE)
)
#Set XGBoost grid
grid <- dials::grid_max_entropy(params, size = gridNumber)
wflow <- workflowFunc(mod = mod,
formula = formula,
folds = folds,
grid = grid,
evalMetric = evalMetric,
type = "binary class")
output <- trainTestEvalClassif(final = wflow$final,
train = train,
test = test,
response = response)
output$tune <- wflow$tune
if(calcFeatImp == TRUE) {
featImp <- featImpCalc(final = wflow$final,
train = train,
response = response,
evalMetric = evalMetric)
#Add variables to output
output$featImpPlot <- featImp$plot
output$featImpVars <- featImp$vars
}
return(output)
}
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
use_r("lgbmRegBinary.R")
require(devtools)
use_r("lgbmRegBinary.R")
document()
rm(list = c("lgbmBinaryClassif"))
document()
document()
install()
require(easytidymodels)
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
document()
install()
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
require(easytidymodels)
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
document()
install()
require(easytidymodels)
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
install.packages('lightgbm')
lgbmClass <- lgbmBinaryClassif(
recipe = rec,
response = resp,
folds = folds,
train = train_df,
test = test_df,
evalMetric = "roc_auc"
)
rlang::last_error()
rlang::last_trace()
#remotes::install_github("curso-r/treesnip")
require(treesnip)
formula <- stats::as.formula(paste(response, ".", sep="~"))
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Create model
mod <- parsnip::boost_tree(min_n = tune(),
tree_depth = tune(),
mtry = tune(),
trees = treeNum) %>%
parsnip::set_engine("lightgbm")
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::min_n(),
dials::tree_depth(),
dials::finalize(dials::mtry(),dplyr::select(train,-matches(response)), force = TRUE)
)
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::min_n(),
dials::tree_depth(),
dials::finalize(dials::mtry(),dplyr::select(train_df,-matches(response)), force = TRUE)
)
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::min_n(),
dials::tree_depth(),
dials::finalize(dials::mtry(),dplyr::select(train_df,-matches(resp)), force = TRUE)
)
#Set XGBoost grid
grid <- dials::grid_max_entropy(params, size = gridNumber)
#Set XGBoost grid
grid <- dials::grid_max_entropy(params, size = 10)
#Set LightGBM grid
grid <- dials::grid_max_entropy(params, size = 10)
wflow <- workflows::workflow() %>%
workflows::add_model(mod) %>%
workflows::add_formula(formula)
tune <- tune::tune_grid(
wflow,
resamples = folds,
grid      = grid,
metrics   = yardstick::metric_set(
yardstick::bal_accuracy,
yardstick::mn_log_loss,
yardstick::kap,
yardstick::roc_auc,
yardstick::mcc,
yardstick::precision,
yardstick::recall,
yardstick::sens,
yardstick::spec),
control   = tune::control_grid(verbose = TRUE,
save_pred = TRUE,
save_workflow = TRUE,
allow_par = TRUE,
parallel_over = "everything")
)
tune$.notes
#Create model
mod <- parsnip::boost_tree(mtry = 1,
trees = 100) %>%
parsnip::set_engine("lightgbm")
wflow <- workflows::workflow() %>%
workflows::add_model(mod) %>%
workflows::add_formula(formula)
fit_resamples()
?fit_resamples
tune <- tune::fit_resamples(
wflow,
resamples = folds,
metrics   = yardstick::metric_set(
yardstick::bal_accuracy,
yardstick::mn_log_loss,
yardstick::kap,
yardstick::roc_auc,
yardstick::mcc,
yardstick::precision,
yardstick::recall,
yardstick::sens,
yardstick::spec),
control   = tune::control_resamples(verbose = TRUE,
save_pred = TRUE,
save_workflow = TRUE,
allow_par = TRUE,
parallel_over = "everything")
)
tune$.notes
require(lightgbm)
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Create model
mod <- parsnip::boost_tree(min_n = tune(),
tree_depth = tune(),
mtry = tune(),
trees = 100) %>%
parsnip::set_engine("lightgbm")
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::min_n(),
dials::tree_depth(),
dials::finalize(dials::mtry(),dplyr::select(train_df,-matches(resp)), force = TRUE)
)
#Set XGBoost grid
grid <- dials::grid_max_entropy(params, size = 10)
wflow <- workflows::workflow() %>%
workflows::add_model(mod) %>%
workflows::add_formula(formula)
tune <- tune::tune_grid(
wflow,
resamples = folds,
grid      = grid,
metrics   = yardstick::metric_set(
yardstick::bal_accuracy,
yardstick::mn_log_loss,
yardstick::kap,
yardstick::roc_auc,
yardstick::mcc,
yardstick::precision,
yardstick::recall,
yardstick::sens,
yardstick::spec),
control   = tune::control_grid(verbose = TRUE,
save_pred = TRUE,
save_workflow = TRUE,
allow_par = TRUE,
parallel_over = "everything")
)
tune$.notes
install.packages("catboost")
install.packages("torch")
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Create model
mod <- parsnip::boost_tree(min_n = tune(),
tree_depth = tune(),
mtry = tune(),
trees = 100) %>%
parsnip::set_engine("catboost")
#Set lightGBM parameters to tune
params <- dials::parameters(
dials::min_n(),
dials::tree_depth(),
dials::finalize(dials::mtry(),dplyr::select(train_df,-matches(resp)), force = TRUE)
)
#Set XGBoost grid
grid <- dials::grid_max_entropy(params, size = 10)
wflow <- workflows::workflow() %>%
workflows::add_model(mod) %>%
workflows::add_formula(formula)
tune <- tune::tune_grid(
wflow,
resamples = folds,
grid      = grid,
metrics   = yardstick::metric_set(
yardstick::bal_accuracy,
yardstick::mn_log_loss,
yardstick::kap,
yardstick::roc_auc,
yardstick::mcc,
yardstick::precision,
yardstick::recall,
yardstick::sens,
yardstick::spec),
control   = tune::control_grid(verbose = TRUE,
save_pred = TRUE,
save_workflow = TRUE,
allow_par = TRUE,
parallel_over = "everything")
)
install.packages("catboost")
install.packages("tabnet")
require(tabnet)
mod <- tabnet(epochs = 50, batch_size = 128) %>%
set_engine("torch", verbose = TRUE) %>%
set_mode("classification")
wflow <- workflows::workflow() %>%
workflows::add_model(mod) %>%
workflows::add_formula(formula)
fit_rs <- wflow %>%
fit_resamples(folds)
fit_rs$.notes
fit_rs$.notes[[1]]
mod
require(torch)
require(tabnet)
require(torch)
mod <- tabnet(epochs = 50, batch_size = 128) %>%
set_engine("torch", verbose = TRUE) %>%
set_mode("classification")
wflow <- workflows::workflow() %>%
workflows::add_model(mod) %>%
workflows::add_formula(formula)
fit_rs <- wflow %>%
fit_resamples(folds)
fit_rs$.notes
