% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/knnClassif.R
\name{knnClassif}
\alias{knnClassif}
\title{K-Nearest Neighbors Classification}
\usage{
knnClassif(
  response = response,
  recipe = rec,
  folds = folds,
  train = train_df,
  test = test_df,
  gridNumber = 15,
  evalMetric = "bal_accuracy"
)
}
\arguments{
\item{response}{Character. The variable that is the response for analysis.}

\item{recipe}{A recipe object.}

\item{folds}{A rsample::vfolds_cv object.}

\item{train}{Data frame/tibble. The training data set.}

\item{test}{Data frame/tibble. The testing data set.}

\item{gridNumber}{Numeric. The size of the grid to tune on. Default is 15.}

\item{evalMetric}{Character. The classification metric you want to evaluate the model's accuracy on. Default is bal_accuracy. List of metrics available to choose from:
\itemize{
\item bal_accuracy
\item mn_log_loss
\item roc_auc
\item mcc
\item kap
\item sens
\item spec
\item precision
\item recall
}}
}
\value{
A list with the following outputs:
\itemize{
\item Training confusion matrix
\item Training model metric score
\item Testing confusion matrix
\item Testing model metric score
\item Final model chosen
\item Tuned model
}
}
\description{
Fits a K-Nearest Neighbors Classification Model.
}
\details{
Note: tunes the following parameters:
\itemize{
\item neighbors: The number of neighbors considered at each prediction.
\item weight_func: The type of kernel function that weights the distances between samples.
\item dist_power: The parameter used when calculating the Minkowski distance. This corresponds to the Manhattan distance with dist_power = 1 and the Euclidean distance with dist_power = 2.
}
}
\examples{
library(easytidymodels)
library(dplyr)
library(recipes)
utils::data(penguins, package = "modeldata")
#Define your response variable and formula object here
resp <- "sex"
formula <- stats::as.formula(paste(resp, ".", sep="~"))
#Split data into training and testing sets
split <- trainTestSplit(penguins, stratifyOnResponse = TRUE,
responseVar = resp)
#Create recipe for feature engineering for dataset, varies based on data working with
rec <- recipe(formula, data = split$train) \%>\% step_knnimpute(!!resp) \%>\%
step_dummy(all_nominal(), -all_outcomes()) \%>\%
step_medianimpute(all_predictors()) \%>\% step_normalize(all_predictors()) \%>\%
step_dummy(all_nominal(), -all_outcomes()) \%>\% step_nzv(all_predictors()) \%>\%
step_corr(all_numeric(), -all_outcomes(), threshold = .8) \%>\% prep()
train_df <- bake(rec, split$train)
test_df <- bake(rec, split$test)
folds <- cvFolds(train_df)

#knn <- svmClassif(recipe = rec, response = resp, folds = folds,
#train = train_df, test = test_df)

#Confusion Matrix
#knn$trainConfMat

#Plot of confusion matrix
#knn$trainConfMatPlot

#Test Confusion Matrix
#knn$testConfMat

#Test Confusion Matrix Plot
#knn$testConfMatPlot
}
