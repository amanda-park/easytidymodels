---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# easytidymodels

<!-- badges: start -->
<!-- badges: end -->

The goal of easytidymodels is to make running analyses in R using the tidymodels framework even easier. This is custom code I wrote to make the code more reproducible and avoid copy-pasting so often. Note: this is currently a work in progress!

## Installation

You can install easytidymodels like this:

``` r
# install.packages("devtools")
devtools::install_github("amanda-park/easytidymodels")
```

## Preparing Data for Analysis

This is a basic example of one splitting data in the package. 

* The function trainTestSplit is a wrapper for rsample's function that allow you to nicely split up your data into training and testing sets. For reusability's sake it has been put into a function here. 
* The function cvFolds is a wrapper for rsample's vfold_cv.
* The function createRecipe just creates a simple recipe of your dataset. If more advanced recipes are required, I recommend calling recipe() and creating one specific to your dataset's needs.


```{r trainTestSplit}
library(easytidymodels)

#Simulate data
df <- data.frame(var1 = as.factor(c(rep(1, 50), rep(0, 50))),
                 var2 = rnorm(100),
                 var3 = c(rnorm(55), rnorm(45, 5)),
                 var4 = rnorm(100),
                 var5 = c(rnorm(60), rnorm(40, 3)),
                 var6 = as.factor(c(rep(0, 20), rep(1, 40), rep(2, 40))))

#Set response variable
resp <- "var6"


split <- trainTestSplit(data = df, 
                        responseVar = resp)

#Create simple recipe object
rec <- createRecipe(split$train, 
                    responseVar = resp)

#Create training, testing, and bootstrapped data sets
train_df <- recipes::bake(rec, split$train)
test_df <- recipes::bake(rec, split$test)
boot_df <- split$boot

#Create cross-validation folds
folds <- cvFolds(train_df, 5)
```

## Classification Examples

### Logistic Regression

Tunes both the penalty and mixture terms, fits a model based on the classification evaluation metric specified (default bal_accuracy), and returns an evaluation of the model on both the training and testing data.

```{r logReg}
# #Run logistic regression - only commented to avoid readme error
# lr <- logRegBinary(recipe = rec,
#                    response = resp,
#                    folds = folds,
#                    train = train_df,
#                    test = test_df)

# #Shows training and testing data confusion matrix
# lr$trainConfMat
# lr$testConfMat
#
# #Shows training data confusion matrix plot
# lr$trainConfMatPlot
# lr$testConfMatPlot
#
# #Shows training data score based on classification metrics
# lr$trainScore
# lr$testScore
#
# #Shows actual predictions for training and testing
# lr$trainPred
# lr$testPred
#
# #Shows tuned model optimized on evaluation metric chosen
# lr$final


```

### KNN Classification

```{r knnClass}
# knnClass <- knnClassif(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   evalMetric = "bal_accuracy"
# )

```

### SVM Classification

```{r svmClass}
# svmClass <- svmClassif(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   evalMetric = "bal_accuracy"
# )

```


### XGBoost

Tunes the following:

   * learn_rate (or eta)
   
   * sample_size (or subsample)
   
   * mtry (or colsample_bytree)
   
   * min_n (or min_child_weight)
   
   * tree_depth (or max_dept)

Fits a model based on the classification evaluation metric specified (default bal_accuracy), returns an evaluation of the model on both the training and testing data, and also returns variable importance for the model.

```{r xgboostClass}
#XGBoost classification
# xgClass <- xgBinaryClassif(
#                    recipe = rec,
#                    response = resp,
#                    folds = folds,
#                    train = train_df,
#                    test = test_df,
#                    evalMetric = "roc_auc"
#                    )
# 
# #All the same functions for logistic regression work here, but also others:
# 
# #Feature importance plot
# xgClass$featImpPlot
# 
# #Feature importance variables
# xgClass$featImpVars
```

## Multiclass Classification

### Multinomial Regression

```{r multinom}
# mr <- logRegMulti(
#                    recipe = rec,
#                    response = resp,
#                    folds = folds,
#                    train = train_df,
#                    test = test_df
#                    )
```

### XGBoost Multiclass Classifcation

```{r xgMulti}
# xgMult <- xgMultiClassif(
#                    recipe = rec,
#                    response = resp,
#                    folds = folds,
#                    train = train_df,
#                    test = test_df
#                    )
```

## Regression

### Linear Regression

```{r linearRegression}
# linReg <- linearRegress(
#   response = resp,
#   data = df,
#   train = train_df,
#   test = test_df,
#   tidyModelVersion = TRUE,
#   recipe = rec,
#   folds = folds,
#   evalMetric = "rmse"
# )
```

### MARS

```{r mars}
# mars <- marsRegress(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   evalMetric = "mae"
# )

```

### XGBoost

```{r xgRegress}
# xgReg <- xgRegress(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   calcFeatImp = TRUE,
#   evalMetric = "mae"
# )
# 
# #Show accuracy metrics testing data
# xgReg$testScore
# 
# #Feature importance plot
# xgReg$featImpPlot

```

### Random Forest Regression

```{r randomForest}
# rfReg <- rfRegress(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   calcFeatImp = TRUE,
#   evalMetric = "mae"
# )
```

### Support Vector Machine

```{r svm}
# svmReg <- svmRegress(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   evalMetric = "rmse"
# )

```

### KNN Regression

```{r knn}
# knnReg <- knnRegress(
#   recipe = rec,
#   response = resp,
#   folds = folds,
#   train = train_df,
#   test = test_df,
#   evalMetric = "rmse"
# )
```


